@misc{resnet50,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}

@misc{alexnet,
      title={One weird trick for parallelizing convolutional neural networks}, 
      author={Alex Krizhevsky},
      year={2014},
      eprint={1404.5997},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1404.5997}, 
}

@online{gradient,
    title = {CS231n Convolutional Neural Networks for Visual Recognition},
    url = {https://cs231n.github.io/}
}

@online{sgd,
    title = {An overview of gradient descent optimization algorithms},
    url = {https://www.ruder.io/optimizing-gradient-descent/\#gradientdescentvariants}
}

@online{pytorch,
    title = {Pytorch},
    url = {https://pytorch.org/},
}

@inproceedings{checkfreq,
    author = {Jayashree Mohan and Amar Phanishayee and Vijay Chidambaram},
    title = {{CheckFreq}: Frequent, {Fine-Grained} {DNN} Checkpointing},
    booktitle = {19th USENIX Conference on File and Storage Technologies (FAST
                 21)},
    year = {2021},
    isbn = {978-1-939133-20-5},
    pages = {203--216},
    url = {https://www.usenix.org/conference/fast21/presentation/mohan},
    publisher = {USENIX Association},
    month = feb,
}

@inproceedings{8638422,
    author = {Chien, Steven W. D. and Markidis, Stefano and Sishtla, Chaitanya
              Prasad and Santos, Luis and Herman, Pawel and Narasimhamurthy, Sai
              and Laure, Erwin},
    booktitle = {2018 IEEE/ACM 3rd International Workshop on Parallel Data
                 Storage \& Data Intensive Scalable Computing Systems
                 (PDSW-DISCS)},
    title = {Characterizing Deep-Learning I/O Workloads in TensorFlow},
    year = {2018},
    volume = {},
    number = {},
    pages = {54-63},
    keywords = {Training;Pipelines;Checkpointing;Prefetching;Benchmark
                testing;Google;parallel-I/O;Input-pipeline;Deep-Learning;Tensorflow
                },
    doi = {10.1109/PDSW-DISCS.2018.00011},
}

@inproceedings{nvme,
    author = {Zhu, Yue and Yu, Weikuan and Jiao, Bing and Mohror, Kathryn and
              Moody, Adam and Chowdhury, Fahim},
    booktitle = {2019 IEEE International Conference on Cluster Computing
                 (CLUSTER)},
    title = {Efficient User-Level Storage Disaggregation for Deep Learning},
    year = {2019},
    volume = {},
    number = {},
    pages = {1-12},
    keywords = {Nonvolatile memory;Training;Deep
                learning;Fabrics;Protocols;Throughput;Performance evaluation},
    doi = {10.1109/CLUSTER.2019.8891023},
}

@article{LMDB,
    author = {Pumma, Sarunya and Si, Min and Feng, Wu-Chun and Balaji, Pavan},
    title = {Scalable Deep Learning via I/O Analysis and Optimization},
    year = {2019},
    issue_date = {June 2019},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {6},
    number = {2},
    issn = {2329-4949},
    url = {https://doi.org/10.1145/3331526},
    doi = {10.1145/3331526},
    abstract = {Scalable deep neural network training has been gaining
                prominence because of the increasing importance of deep learning
                in a multitude of scientific and commercial domains. Consequently
                , a number of researchers have investigated techniques to
                optimize deep learning systems. Much of the prior work has
                focused on runtime and algorithmic enhancements to optimize the
                computation and communication. Despite these enhancements,
                however, deep learning systems still suffer from scalability
                limitations, particularly with respect to data I/O. This
                situation is especially true for training models where the
                computation can be effectively parallelized, leaving I/O as the
                major bottleneck. In fact, our analysis shows that I/O can take
                up to 90\% of the total training time. Thus, in this article, we
                first analyze LMDB, the most widely used I/O subsystem of deep
                learning frameworks, to understand the causes of this I/O
                inefficiency. Based on our analysis, we propose LMDBIO—an
                optimized I/O plugin for scalable deep learning. LMDBIO includes
                six novel optimizations that together address the various
                shortcomings in existing I/O for deep learning. Our experimental
                results show that LMDBIO significantly outperforms LMDB in all
                cases and improves overall application performance by up to
                65-fold on a 9,216-core system.},
    journal = {ACM Trans. Parallel Comput.},
    month = jul,
    articleno = {6},
    numpages = {34},
    keywords = {parallel I/O, Scalable deep learning, LMDBIO, LMDB, I/O in deep
                learning, I/O bottleneck, Caffe},
}

@inproceedings{9229605,
    author = {Chien, Steven W. D. and Podobas, Artur and Peng, Ivy B. and
              Markidis, Stefano},
    booktitle = {2020 IEEE International Conference on Cluster Computing
                 (CLUSTER)},
    title = {tf-Darshan: Understanding Fine-grained I/O Performance in Machine
             Learning Workloads},
    year = {2020},
    volume = {},
    number = {},
    pages = {359-370},
    keywords = {Training;Runtime library;Instruments;Data visualization;Machine
                learning;Tools;Optimization;Deep-Learning;Machine
                Learning;I/O;Data pre-processing;TensorFlow;profiling;tracing},
    doi = {10.1109/CLUSTER49012.2020.00046},
}

@inproceedings{zoomin,
    author = {Wang, Teng and Byna, Suren and Lockwood, Glenn K. and Snyder,
              Shane and Carns, Philip and Kim, Sunggon and Wright, Nicholas J.},
    booktitle = {2019 19th IEEE/ACM International Symposium on Cluster, Cloud
                 and Grid Computing (CCGRID)},
    title = {A Zoom-in Analysis of I/O Logs to Detect Root Causes of I/O
             Performance Bottlenecks},
    year = {2019},
    volume = {},
    number = {},
    pages = {102-111},
    keywords = {
                Tools;Servers;Bandwidth;Meteorology;Monitoring;Metadata;Instruments;Darshan;Lustre
                Monitoring Tools;Slurm;IO Analysis;IO Trace},
    doi = {10.1109/CCGRID.2019.00021},
}

@inproceedings{10.1145/3337821.3337902,
    author = {Chowdhury, Fahim and Zhu, Yue and Heer, Todd and Paredes, Saul and
              Moody, Adam and Goldstone, Robin and Mohror, Kathryn and Yu,
              Weikuan},
    title = {I/O Characterization and Performance Evaluation of BeeGFS for Deep
             Learning},
    year = {2019},
    isbn = {9781450362955},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3337821.3337902},
    doi = {10.1145/3337821.3337902},
    abstract = {Parallel File Systems (PFSs) are frequently deployed on
                leadership High Performance Computing (HPC) systems to ensure
                efficient I/O, persistent storage and scalable performance.
                Emerging Deep Learning (DL) applications incur new I/O and
                storage requirements to HPC systems with batched input of small
                random files. This mandates PFSs to have commensurate features
                that can meet the needs of DL applications. BeeGFS is a recently
                emerging PFS that has grabbed the attention of the research and
                industry world because of its performance, scalability and ease
                of use. While emphasizing a systematic performance analysis of
                BeeGFS, in this paper, we present the architectural and system
                features of BeeGFS, and perform an experimental evaluation using
                cutting-edge I/O, Metadata and DL application benchmarks.
                Particularly, we have utilized AlexNet and ResNet-50 models for
                the classification of ImageNet dataset using the Livermore Big
                Artificial Neural Network Toolkit (LBANN), and ImageNet data
                reader pipeline atop TensorFlow and Horovod. Through extensive
                performance characterization of BeeGFS, our study provides a
                useful documentation on how to leverage BeeGFS for the emerging
                DL applications.},
    booktitle = {Proceedings of the 48th International Conference on Parallel
                 Processing},
    articleno = {80},
    numpages = {10},
    location = {Kyoto, Japan},
    series = {ICPP '19},
}

@inproceedings{initial,
    title = {Initial characterization of i/o in large-scale deep learning
             applications},
    author = {Chowdhury, Fahim and Liu, Jialin and Koziol, Quincey and Kurth,
              Thorsten and Farrell, Steven and Byna, Suren and Yu, W},
    booktitle = {3rd Joint International Workshop on Parallel Data Storage and
                 Data Intensive Scalable Computing Systems (PDSW-DISCS’18) at the
                 International Conference on High Performance Computing,
                 Networking, Storage and Analysis},
    year = {2018},
}

@ARTICLE{eBPFSurvey,
  author={Sharaf, Husain and Ahmad, Imtiaz and Dimitriou, Tassos},
  journal={IEEE Access}, 
  title={Extended Berkeley Packet Filter: An Application Perspective}, 
  year={2022},
  volume={10},
  number={},
  pages={126370-126393},
  keywords={Storage management;Band-pass filters;Filtering;Security;Cloud computing;Monitoring;Linux;Containers;BPF;eBPF;XDP;Linux kernel;security;network;sandboxing;storage;containers},
  doi={10.1109/ACCESS.2022.3226269}}

% Background - eBPF

@online{bpftrace,
    title = {bpftrace},
    url = {https://github.com/iovisor/bpftrace},
}

@book{bgreggBook,
  title={BPF performance tools},
  author={Gregg, Brendan},
  year={2019},
  publisher={Addison-Wesley Professional}
}

% Related work - Oana

@Thesis{OanaDL,
	title = {Characterizing and {Modelling} the {I}/{O} {Patterns} of {Deep} {Learning} {Training} {Workloads}},
	url = {https://escholarship.mcgill.ca/concern/theses/v979v8126},
	language = {http://id.loc.gov/vocabulary/iso639-2/eng},
	urldate = {2024-11-27},
	author = {Ho-Von, Loic},
	collaborator = {Balmau (Supervisor), Oana},
	note = {Publisher: McGill University},
	keywords = {Computer Science},
	file = {Thesis | Characterizing and Modelling the I/O Patterns of Deep Learning Training Workloads | ID\: v979v8126 | eScholarship@McGill:/home/hiddenduck/Zotero/storage/45942WXR/v979v8126.html:text/html},
}

@article{OanaML,
author = {Balmau, Oana},
title = {Characterizing I/O in Machine Learning with MLPerf Storage},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/3572751.3572765},
doi = {10.1145/3572751.3572765},
abstract = {Data is the driving force behind machine learning (ML) algorithms. The way we ingest, store, and serve data can impact the performance of end-to-end training and inference significantly [11]. However, efficient storage and pre-processing of training data has received far less focus in ML compared to efforts in building specialized software frameworks and hardware accelerators. The amount of data that we produce is growing exponentially, making it expensive and difficult to keep entire training datasets in main memory. Increasingly, ML algorithms will need to access data from persistent storage in an efficient way.},
journal = {SIGMOD Rec.},
month = nov,
pages = {47-48},
numpages = {2}
}

% related work, ML/DL characterization in HPC

@INPROCEEDINGS{CharcterizationMLIOLeadHPC,

  author={Paul, Arnab K. and Karimi, Ahmad Maroof and Wang, Feiyi},

  booktitle={2021 29th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)}, 

  title={Characterizing Machine Learning I/O Workloads on Leadership Scale HPC Systems}, 

  year={2021},

  volume={},

  number={},

  pages={1-8},

  keywords={Leadership;Analytical models;Systematics;File systems;Computational modeling;High performance computing;Tools;Burst Buffer;Darshan;High Performance Computing;HPC Storage;IBM Spectrum Scale;I/O Characterization;Machine Learning;Parallel File System},

  doi={10.1109/MASCOTS53633.2021.9614303}
}

@article{UnderstandingDLIOHPC,
  title={Understanding I/O behavior of Scientific Deep Learning Applications in HPC systems},
  author={Devarajan, Hariharan and Zheng, Huihuo and Sun, Xian-He and Vishwanath, Venkatram}
}

@INPROCEEDINGS {DFTracerAIHPC,
author = { Devarajan, Hariharan and Pottier, Loïc and Velusamy, Kaushik and Zheng, Huihuo and Yildirim, Izzet and Kogiou, Olga and Yu, Weikuan and Kougkas, Anthony and Sun, Xian-He and Yeom, Jae Seung and Mohror, Kathryn },
booktitle = { 2024 SC24: International Conference for High Performance Computing, Networking, Storage and Analysis SC },
title = {{ DFTracer: An Analysis-Friendly Data Flow Tracer for AI-Driven Workflows }},
year = {2024},
volume = {},
ISSN = {},
pages = {224-247},
abstract = { Modern HPC workflows involve intricate coupling of simulation, data analytics, and artificial intelligence (AI) applications to improve time to scientific insight. These workflows require a cohesive set of performance analysis tools to provide a comprehensive understanding of data exchange patterns in HPC systems. However, current tools are not designed to work with an AI-based I/O software stack that requires tracing at multiple levels of the application. To this end, we developed a data flow tracer called DFTracer to capture data-centric events from workflows and the I/O stack to build a detailed understanding of the data exchange within AI-driven workflows. DFTracer has the following three novel features, including a unified interface to capture trace data from different layers in the software stack, a trace format that is analysis-friendly and optimized to support efficiently loading multi-million events in a few seconds, and the capability to tag events with workflow-specific context to perform domain-centric data flow analysis for workflows. Additionally, we demonstrate that DFTracer has a 1.44x smaller runtime overhead and 1.3-7.1x smaller trace size than state-of-the-art tracing tools such as Score-P, Recorder, and Darshan. Moreover, with AI-driven workflows, Score-P, Recorder, and Darshan cannot find I/O accesses from dynamically spawned processes, and their load performance of 100M events is three orders of magnitude slower than DFTracer. In conclusion, we demonstrate that DFTracer can capture multi-level performance data, including contextual event tagging with a low overhead of 1-5% from AI-driven workflows such as MuMMI and Microsoft’s Megatron Deepspeed running on large-scale HPC systems. },
keywords = {deep learning;workflows;I/O;tracer;multilevel;application apis;system calls;transparent;interception},
doi = {10.1109/SC41406.2024.00023},
url = {https://doi.ieeecomputersociety.org/10.1109/SC41406.2024.00023},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Nov}



% Normal HPC characterization

@INPROCEEDINGS{HPCIODarshan,
  author={Snyder, Shane and Carns, Philip and Harms, Kevin and Ross, Robert and Lockwood, Glenn K. and Wright, Nicholas J.},
  booktitle={2016 5th Workshop on Extreme-Scale Programming Tools (ESPT)}, 
  title={Modular HPC I/O Characterization with Darshan}, 
  year={2016},
  volume={},
  number={},
  pages={9-17},
  keywords={Instruments;Libraries;Buffer storage;Production systems;Writing;Runtime;Registers},
  doi={10.1109/ESPT.2016.006}
}

@INPROCEEDINGS{HPCIO24/7,
  author={Carns, Philip and Latham, Robert and Ross, Robert and Iskra, Kamil and Lang, Samuel and Riley, Katherine},
  booktitle={2009 IEEE International Conference on Cluster Computing and Workshops}, 
  title={24/7 Characterization of petascale I/O workloads}, 
  year={2009},
  volume={},
  number={},
  pages={1-10},
  keywords={Computer applications;Laboratories;Memory management;Reflection;File systems;Petascale computing;Mathematics;Computer science;Application software;Pressing},
  doi={10.1109/CLUSTR.2009.5289150}
}


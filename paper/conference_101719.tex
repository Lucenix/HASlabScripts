\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\bibliographystyle{IEEEtran}

\begin{document}

\title{Characterization of Patterns in Deep Learning Models using eBPF\\
}

\author{\IEEEauthorblockN{André Lucena Ribas Ferreira}
	\IEEEauthorblockA{
		\textit{University of Minho}\\
		Braga, Portugal \\
		pg52672@uminho.pt}
	\and
	\IEEEauthorblockN{Carlos Eduardo da Silva Machado}
	\IEEEauthorblockA{
		\textit{University of Minho}\\
		Braga, Portugal \\
		pg52675@uminho.pt}
	\and
	\IEEEauthorblockN{Gonçalo Manuel Maia de Sousa}
	\IEEEauthorblockA{
		\textit{University of Minho}\\
		Braga, Portugal \\
		pg52682@uminho.pt}
}

\maketitle

\begin{abstract}
	This document is a model and instructions for \LaTeX.
	This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes,
	or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
	component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
In recent years Machine Learning has seen a massive increase in popularity.
Specifically Deep Learning (DL) has been in several tasks such as image classification
and natural language processing.\\
There have been several studies that analyze the I/O patterns of DL Workflows and this work
has been used to produce useful assumptions about DL I/O patterns. These have been commonly used to
justify various types of optimization explorations \cite{LMDB, nvme, beegfsDL, bamboo}, and inform 
other applications such as benchmarks specific to DL \cite{TFbenchmark}. \\
Application-specific I/O characterization is therefore a requirement for creating
unique and worthwhile solutions to
newer performance problems, as well as, for example reducing the monetary cost of training an 
DL model \cite{bamboo}, however very few of the existing I/O characterization tools
can get down to the kernel level, instead opting, for example for intercepting system calls \cite{tfdarshan}, this approach leaves out important information about whats happening at levels closer to the operating system.\\
\textbf{eBPF's} are a tool that allows for programs to be run on the kernel without changes to
the source code and without having to reboot the kernel for code changes to take effect.\\
\textbf{Contributions.} The main contributions of this paper are as follows
\begin{itemize}
    \item To provide a methodology for I/O characterization of DL, including methods for collecting data using existing and custom tools based on eBPF's, and tools to gather and visualize the collected data.
    \item Provide useful insights about the I/O patterns of certain DL workflows based on an analyses of the data collected.
\end{itemize}

% \begin{itemize}
% 	\item Machine learning has increased in popularity
% 	\item Deep Learning is a subset of Machine Learning
% 	      \begin{itemize}
% 		      \item image classification
% 		      \item natural language processing
% 	      \end{itemize}
% 	\item studies have tried to analyze I/O patterns in DL Workflows (source)
% 	\item \item I/O Characterization has served to produce diverse assumptions about DL I/O behavior, like high overhead for random reads, which are commonly used to justify various types of optimization explorations (\cite{LMDB}, \cite{nvme}, \cite{beegfs}), and inform other applications based on them, like benchmarks specific to DL \cite{TFbenchmark}. Application-specific storage characterization is required to find unique and worthwhile solutions for newer performance problems.
% 	\item very few get down to kernel level
% 	\item eBPF are ...
% 	\item we seek to provide a tool to Characterize DL workloads using eBPF's
% \end{itemize}

\section{Background}

% passing the chosen data as input through the entire network layer by layer, a method called the forward pass. Afterwards, a chosen loss function compares the difference the network's output and the known ground truth, and the gradient of this function is computed in regard to the network's weights. This step is named backpropagation. The gradients are then used to update the weights of all the layers, converging to a minimization of the loss function, through an algorithm called Gradient Descent, that uses the entire dataset as the input. Each pass of the entire dataset is called an epoch, and several are computed in order to converge to the minimum. \cite{gradient} acho que isto está muito grande e fala pouco do I/O

%However, this approach is extremely costly due to the aforementioned dimension of the dataset, so state-of-the-art algorithms use Stochastic Gradient Descent (SGD) instead, an approximation that breaks an epoch into iterations, that go through the only a subset of the dataset, called a \textit{mini-batch}, as input, ensuring that each data sample is iterated through only once \cite{sgd}. Even if the original SGD only fetches a single data sample per iteration, the number of samples has been increasing over the last years, from 32, to 256 and ultimately to the 80.000s, in order to better utilize the increasing computational power afforded by the widespread utilization of GPUs as accelerators, but at an increased I/O load per iteration \cite{nvme}.

In this section, we elaborate on both the Deep Learning algorithms we seek to analyze, with focus on their I/O description, and the eBPF tools we'll use for that end.

\subsection{Deep Learning}

Deep learning (DL) is a subset of machine learning, characterized by its bigger size and heterogeneity of both algorithms and training datasets, needed for accurate predictions \cite{gradient}. State-of-the-art DL utilizes the Stochastic Gradient Descent (SGD) method, which consists in breaking down sequential training epochs in iterations, wherein subsets of the entire dataset, called \textit{mini-batches}, are read, preprocessed in memory and fed to the algorithm. To reduce training time, this computation may be done in accelerators, like GPUs, and may also be overlapped by the parallel I/O of future batches. For maximum accuracy, each data sample is iterated through only once in each epoch and each batch is shuffled, that is, it is composed of random data samples.

To further accelerate the training process, the dataset can be distributed among several workers in Distributed Data Parallel Learning. The model parameters are cloned among all the workers, each of them iterates through a subset of the data, and then the gradients are averaged and shared with all the workers. This way, all replicas of the network are updated with the same gradients, ensuring all workers remain consistent with each other.

As models get larger and more nodes are used in training, especially in High Performance Computing (HPC) centers, fault tolerance becomes an important issue. To address it, it is common for the model state to be saved periodically to persistent storage. Normally, this is done at epoch boundaries \cite{checkfreq}, which stalls the algorithm, waiting for I/O, whose size depends on the dimension of the network. In case of failure, the model state can be loaded from storage and training can be restarted.

Various frameworks exist to facilitate the Implementation of DL models, and each one tackles I/O differently. \textit{Pytorch} \cite{pytorch} is an up-and-coming open source AI training library, reportedly becoming commonplace in HPC centers. It provides utilities for operating on tensors with GPU support; various customizable data loading and checkpointing specifications, like parallel data treatment by different workers; a library of ready-to-use DNN models; and support for data parallel training of AI models. Its data structure is analogous to those used in other frameworks, like Tensorflow, and common databases used for DL, like LMDB, as it is based on NumPy, which uses memory mapping for its metadata access \cite{LMDB}.

\subsection{I/O in Deep Learning}

DL's I/O patterns have been attributed as the cause for the increasing storage bottleneck found recently in these applications \cite{beegfs}. On the one hand, the usual dataset's average sample is very small, but also very numerous. For example, 75\% of ImageNet's samples are smaller than 147 KB, while the dataset itself consists of over 14 million images. Allied with the random choosing of samples for batches and the scan nature of each epoch's full dataset traversal, these patterns don't allow for the typical optimizations of traditional storage systems, directed at large and sequential patterns, like prefetching and caching \cite{initial}. On the other hand, the number of samples used for each batch has been increasing over the last years, from 32, to 256 and, in some cases, to the 80.000s, in order to better utilize the higher computational power \cite{nvme}. This comes at an increased I/O load per iteration, while also reducing the overlap between I/O and computation, as GPU utilization tends to scale better than I/O performance when the input sizes are bigger \cite{TFbenchmark}. 

Although DL is based primarily upon a read workload, when checkpointing is enabled there are moments of heavy writes. Checkpointing sizes depend on the size of the network, as they can be defined with wholly different numbers of layers and weights, which impacts differently the overall distribution.

\subsection{eBPF}

\textit{Extended Berkeley Packet Filter} (eBPF) is a technology that has attracted massive adoption by both industry and academia, with a wide range of applications such as network, storage, security and sandboxing.
The main advantages are being able to run programs in kernel without modifying the kernel source code nor rebooting the system in order to take effect \cite{eBPFSurvey}.

To facilitate the creation, compilation and execution of eBPF programs, bpftrace \cite{bpftrace}, a high-level tracing language inspired in DTrace, can be used in conjunction to a wide collection of tools, some are also available in \cite{bgreggBook}.

\section{Related work}

% Machine Learning in and out of HPC

HPC centers' wide array of hosted scientific applications has fostered a need for quality and agnostic I/O pattern identification. Other studies have developed tools for that end, like Darshan \cite{HPCIO24/7}, that seeks to transparently, through \texttt{LD\_PRELOAD}, classify the per-file access statistics of different processes of a given application, while minimally impacting the system it is running on. Various more specialized approaches have built upon Darshan as a base, in order to better depict a distributed environment. These include Wang et al. \cite{zoomin} and their top-down analysis, from system-wide, using HPC's specific monitoring tools Lustre Monitoring Toolkit and Slurm Logs, to single-job, using Darshan, while also introducing two techniques for I/O analysis, a correlation matrix between runs with different parameters and sweep-line based analysis of file access bottlenecks; or Darshan's own HPC focused enhancement \cite{HPCIODarshan}. 

However, with DL applications gaining ground in HPC centers, studies have adapted to specific analysis of their I/O traits. Some have kept Darshan as their backbone, either to conduct general DL pattern descriptions, with focus on HPC's response to those workloads (\cite{IOHPCDLBOOK,CharacterizationMLIOLeadHPC, UnderstandingDLIOHPC}), or to develop more advanced tools for that end (\cite{DFTracerAIHPC}). TF-Darshan \cite{tfdarshan} is one of those tools, transparently integrating Darshan with the DL framework Tensorflow's own profiler TensorBoard through runtime linkage, although it doesn't integrate with any other DL frameworks. Others sought to explore the performance bottlenecks around DL's data usage, be it at the database level, for example, LMDBIO \cite{LMDB}, a proposed optimization for LMDB, one of the most used DL databases, and its \texttt{mmap} and randomization related shortcomings, or be it at the file system layer, especially considering parallel systems at HPC centers, proposing alternatives like BeeGFS \cite{beegfs} and ways to leverage it towards DL I/O optimization \cite{beegfsDL}.

Due to DL's overwhelming majority of read operations, the most common examined metrics include the I/O file skew and full read time per process; the read bandwidth for each block size and the number of context switches happening on I/O blocked processes. We've identified some gaps in all analysis overall, like the lack of focus on cache utilization during the epoch dataset scan, lack of network characterization in Distributed Learning communication between workers and, most of all, lack of full I/O stack analysis, considering each step of the training process, over time. Additionally, Darshan itself lacks functionality to adequately track \texttt{mmap} based data manipulation, like \texttt{msync} calls, without additional, intrusive extensions \cite{tfdarshan}, which limits their interaction with DL's most used data representation techniques \cite{LMDB}. 

% Observation through eBPFs
In terms of eBPF, there are some tools that are used for system tracing and performance analysis,
such as DIO \cite{DIO} that allow the observation of I/O interactions between the application and in-kernel storage,
\cite{CAT} which is able to correlate the flow of storage and network requests application and \cite{tracee} that provides runtime security and forensics.  

% Machine Learning analysis with eBPFs
There are already some studies that uses eBPF's capability to provide kernel-level information to understand patterns such as \cite{eBPFDLNetwork} that
leverages eBPF to monitor distributed deep learning training (DDL), investigating network bandwidth utilization in DDL.
eBPF has also been utilized to characterize machine and deep learning workloads, focusing on storage and I/O efficiency in training,
revealing how storage interactions impact overall performance \cite{OanaDL, OanaML}.

\section{Design}
The system is composed of three main components:
\begin{itemize}
	\item \textit{Collector}. Allows collecting system information during model training.
	\item \textit{Backend}. Offers a method for storing, searching and analyzing the raw data collected.
	\item \textit{Visualizer}. Provides a way to represent the data collected through different graphical formats in order to facilitate the understanding and analysis of complex logs by transforming them into more intuitive visual representations.
\end{itemize}
%\textit{Parcer}. Responsible for agreggating the collected data into usable forms, the Parcer is divided into two submodules, the parcer, that aggregates the data collected andd the plotter which  

\begin{figure}[htbp]
    \centering
	\includegraphics[width=0.45\textwidth]{images/DesignPIV2.pdf}
    \caption{Design}
    \label{fig:Design}
\end{figure}

\section{Implementation}
The implementation of the system is divided into the three main components described in the design section.
There are two versions of the system, each using different approaches for data collection, storing and visualization.
Next we overview a more detailed breakdown of how each component was developed and integrated for both versions.
%The collector was implemented using (eBPF's maybe por mais especifico) and ran using GNU screen, the Parser was implemented using python and plotted with the matplotlib and plotly python libraries

\subsection{First Version: bpftrace with python plots} % need a better name ngl

\subsubsection{Collector}

For data collection, we used bpftrace\cite{bpftrace} in conjunction with pre-built tools available in the official repository and the book\cite{bgreggBook}. 
However, we modified most of these tools to better suit DL training, as some of them presented all the collected data together, without any time context.
Others were not detailed enough, capturing system-level metrics instead of focusing on the deep learning training process. 
Additionally, we created custom tools to improve data collection. 
Furthermore, we used a GNU Screen \cite{screen} session to allow the scripts to run in the background during long training sessions.

\subsubsection{Backend}
The raw data is stored in the XFS file system, which ensures efficient handling of large datasets. This is the same file system where our dataset is hosted in. 
We parse the bpftrace\cite{bpftrace} logs by reading the stored files, and then the data is processed and aggregated into structured formats for analysis,
some are plotted directly for visualization and stored in pdf and html formats, others are parsed and stored in the pickle format for direct query access or loading of the interactive visualizations.

\subsubsection{Visualizer}

For visualizing the data, we use Matplotlib, Seaborn, and Plotly from Python.
Matplotlib and Seaborn are primarily used to generate heatmaps, while Plotly is employed for creating different interactive visualizations.

\subsubsection{Limitations}

Some limitations of this version include the need to build and maintain the entire pipeline using handmade scripts, to connect the different tools and ways of execution.
Additionally, if new forms of visualization are required, the parser and visualizer must be adapted to handle different bpftrace log types or generate new types of visualizations.
Other limitations include the scalability of the system, as the dataset size or model complexity increases, the current data collection and processing pipeline may face performance bottlenecks.
Also, the custom modifications made to the bpftrace tools may limit the flexibility of the system, as it could require significant adjustments to apply the pipeline to other models, setups and ML/DL frameworks.

\subsection{Second Version: Grafana pcp-bpftrace}

\subsubsection{Collector}

\subsubsection{Backend}

\subsubsection{Visualizer}

\subsubsection{Limitations}

\section{Evaluation}

\subsection{Experimental Setup}

We analyzed the training of the large network ResNet50 \cite{resnet50} and the smaller AlexNet \cite{alexnet}, whose models were
provided by \textit{PyTorch}, where they're defined with 50 and 13 layers, respectively. Our testing machine is equipped with an Intel® Core™ i9-9900K CPU @ 3.60GHz with
8 physical cores, 16 virtual cores, 16GiB of total RAM and an NVIDIA GeForce RTX 2070 Super TU104 GPU. The dataset we used is the ImageNet \cite{imagenet} dataset, which is originally comprised of 1.2 million sample images, with an average size of 192KiB and a total size of 240GiB. Due to storage technical limitations, we scaled the dataset down to 45GiB total size (of which 36GiB were used for training) by randomly selecting images.

\subsection{Methodology}

\begin{itemize}
	\item dstat, nvidia-smi to get cost of using the tool
	\item python parser and plots
	\item grafana dashboard to get data
\end{itemize}

\subsection{Results}

Tables I and II show general statistics regarding the two models and the different parameters we tested for each.
\\
\textbf{Observation 1.} Training images are read concurrently by multiple processes on training with several workers.
\\
\textbf{Observation 2.} \texttt{fsync()} is not called by any of the processes created by PyTorch, this means that \texttt{torch.save()} does not call \texttt{fsync()}, confirming the assertion made in \cite{checkfreq}.
\\
\textbf{Observation 3.} The vast majority of system calls performed by ResNet50 (3.7 Billion with 2 epochs and batch size 64) are \texttt{sched\_yield()}, these calls are all made by two threads created by the master process, which runs the main function and schedules the model's training.
\\
\textbf{Observation 4.} Checkpoint data was always written to persistent storage when \texttt{torch.save()} was called, confirmed by checking the block I/O write throughput.
\\
\textbf{Observation 5.} When reading from a pipe, processes will read in two times, the first byte and then a larger chunk of data. This results in there being approximately twice as many reads as there are writes to pipes, this pattern is persistent even through changes to the num\_workers parameter in the DataLoader class.
\\
\textbf{Observation 6.} The amount of reads and writes to pipes is proportional to the amount of workers allocated in the DataLoader PyTorch class. When no workers are allocated, no reads or writes to pipes are performed.
\\
\textbf{Observation 7.} The pattern of block read operations is as follows: a large number of initial accesses followed by a gap, a practically complete drop to near 0 reads,  until the moment the first epoch finishes. During the remaining epochs, the accesses are done uniformly. 
\\
\textbf{Observation 8.} This pattern is persistent through changes in the number of workers, however, with fewer workers, the initial spike of accesses lasts for longer, and so the gap is smaller. With zero workers, the gap does not exist. 
\\
\textbf{Observation 9.} This pattern was visible even when the operating system's cache was dropped between epochs.
\\
\textbf{Observation 10.} When writing the checkpoint, the process writes using \texttt{writev} system calls, with the final chunk of data written using a \texttt{write} system call.
\\
\textbf{Observation 11.} 
% \subsubsection{Checkpoint data persistence}
% We observed that no calls to fsync() were performed by the python processes, furthermore, the calls that were performed happened either some time before or some time after the checkpoint was performed, confirming the observation made in \cite{checkfreq} that the torch.save() does not guaranty data persistence.
% We observed however that in all tests performed that the checkpoint data was in fact being persisted to disk.

% \subsubsection{System call usage comparison between models}
% We found that the overwhelming majority(3.7B on tests with 2 epochs and batch size of 64) of system calls performed by the python processes during training of the ResNet50 model were sched\_yield(), when looking at the results segregated by thread we found that ... this is not the case for training AlexNet in the same conditions

% \subsubsection{Read and Write patterns on pipes}
% We analyzed the write and read patterns to pipes of the AlexNet model, we found approximately twice as many reads from pipes as there were writes, upon looking into the pattern we noticed that
% each time a pipe was read from, there was an initial read of 4 bytes followed by a subsequent read of more bytes.
% This pattern remained with variations of the num\_workers parameter for the PyTorch DataLoader class, although the number of operations on pipes reduced in proportion to the number of workers. Conclusively, when this parameter was set to zero workers, there were no reads or writes to pipes.

% \subsubsection{Pattern of block I/O access}
% During the first epoch we found numerous initial reads followed by drop up until the moment the second epoch begins, after which there is a uniform pattern of read's.
% This pattern was consistent with changes to the number of epochs, but the length of the initial burst of read's increased as the number of data loading workers decreased (no workers??).
% This pattern also maintained when the operating systems caches were dropped in between epochs

\section{Conclusion and Future Work}

Future work will certainly involve scaling up the tests done, not only in terms of dataset and model size, but also investing into Distributed Learning, both for Data and Model distribution. eBPF tools permit the tracking of network calls and data transfers, so they aren't limited to tracking only typical disk I/O.

With this analysis fully complete, there is the need to keep up with DL's latest trends. Large Language Models (LLMs) have taken the world by storm, with the public availability of applications like ChatGPT \cite{chatgpt}, and so the training of these models is looking to become the norm on HPC centers. Their smaller datasets and larger models, as well as their different data consumption patterns, compared to those of traditional DL models, possibly bring different patterns into consideration when such examinations are conducted. 

\bibliography{IEEEabrv, refs}

\end{document}

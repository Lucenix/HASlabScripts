\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\bibliographystyle{IEEEtran}

\begin{document}

\title{Conference Paper Title*\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} André Lucena Ribas Ferreira}
    \IEEEauthorblockA{
        \textit{University of Minho}\\
        Braga, Portugal \\
        pg52672@uminho.pt}
    \and
    \IEEEauthorblockN{1\textsuperscript{st} Carlos Eduardo da Silva Machado}
    \IEEEauthorblockA{
        \textit{University of Minho}\\
        Braga, Portugal \\
        pg52675@uminho.pt}
    \and
    \IEEEauthorblockN{1\textsuperscript{st} Gonçalo Manuel Maia de Sousa}
    \IEEEauthorblockA{
        \textit{University of Minho}\\
        Braga, Portugal \\
        pg52682@uminho.pt}
}

\maketitle

\begin{abstract}
    This document is a model and instructions for \LaTeX.
    This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes,
    or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
    component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
\begin{itemize}
    \item Machine learning has increased in popularity
    \begin{itemize}
        \item image classification
        \item natural language processing
    \end{itemize}
    \item studies have tried to analyze I/O patterns in DL Workflows (source)
    \item I/O Characterization has served to produce diverse assumptions about DL I/O behavior, like high overhead for random reads, which are commonly used to justify various types of optimization explorations (\cite{LMDB}, \cite{nvme}, \cite{10.1145/3337821.3337902}). Application-specific storage characterization is required to find unique and worthwhile solutions for newer performance problems.
    \item very few get down to kernel level
    \item eBPF are ...
    \item we seek to provide a tool to Characterize DL workloads using eBPF's
\end{itemize}

\section{Background}

    In this section, we elaborate on both the Deep Learning algorithms we seek to analyze, with focus on their I/O description, and the eBPF tools we'll use for that end.

    \subsection{Deep Learning}

    \textit{Deep learning} (DL) is a subset of machine learning, characterized by its bigger size and heterogeneity of both algorithms and training datasets, needed for accurate predictions \cite{gradient}. State-of-the-art DL utilizes the Stochastic Gradient Descent (SGD) method, which consists in breaking down sequential training epochs in iterations, wherein subsets of the entire dataset, called \textit{mini-batches}, are read, preprocessed in memory and fed to the algorithm. To reduce training time, this computation may be done in accelerators, like GPUs, and may also overlap the I/O of future batches. To maximize accuracy, in each epoch, each data sample is iterated through only once and is chosen randomly to integrate a batch, both patterns that degrade I/O performance. The number of samples of each batch has been increasing over the last years, from 32, to 256 and ultimately to the 80.000s, in order to better utilize the higher computational power but at an increased I/O load per iteration \cite{nvme}.
    % passing the chosen data as input through the entire network layer by layer, a method called the forward pass. Afterwards, a chosen loss function compares the difference the network's output and the known ground truth, and the gradient of this function is computed in regard to the network's weights. This step is named backpropagation. The gradients are then used to update the weights of all the layers, converging to a minimization of the loss function, through an algorithm called Gradient Descent, that uses the entire dataset as the input. Each pass of the entire dataset is called an epoch, and several are computed in order to converge to the minimum. \cite{gradient} acho que isto está muito grande e fala pouco do I/O
    
    %However, this approach is extremely costly due to the aforementioned dimension of the dataset, so state-of-the-art algorithms use Stochastic Gradient Descent (SGD) instead, an approximation that breaks an epoch into iterations, that go through the only a subset of the dataset, called a \textit{mini-batch}, as input, ensuring that each data sample is iterated through only once \cite{sgd}. Even if the original SGD only fetches a single data sample per iteration, the number of samples has been increasing over the last years, from 32, to 256 and ultimately to the 80.000s, in order to better utilize the increasing computational power afforded by the widespread utilization of GPUs as accelerators, but at an increased I/O load per iteration \cite{nvme}.

    To accelerate the training process computation can be distributed among several workers, this is called Distributed Parallel learning and involves cloning
    the model parameters among all the workers, each of the individual iterates through a subset of the data, the gradients are then averaged and shared with all the workers,
    this way, all replicas of the network are updated with the same gradients ensuring all workers remain consistent with each other.

    As models get larger and more nodes are used in training, fault tolerance becomes an important issue, to address this it is common for the model state to be
    saved periodicaly to persistent storage. Normally this is done at epoch boundaries (we can cite check freq here). In case of failure the model state can be loaded from
    storage and training can be restarted. 
    
    \textit{Pytorch} [https://pytorch.org/] is an open source AI training library that provides utilities for operating on tensors with GPU support and dataloading, a library of
    DNN models and suport for dataparallel training of AI models.

    \subsection{eBPF}

    \textit{Extended Berkeley Packet Filter} (eBPF) is a technology that has attracted massive adoption by both industry and academia, with a wide range of applications such as network, storage, security and sandboxing. 
    The main advantages are being able to run programs in kernel without modifying the kernel source code nor rebooting the system in order to take effect \cite{eBPFSurvey}.

    To facilitate the creation, compilation and execution of eBPF programs, bpftrace \cite{bpftrace}, a high-level tracing language inspired in DTrace, can be used in conjunction to a collection of tools, some are also available in \cite{bgreggBook}.
\begin{itemize}
    % \item DL involves iterating multiple times (epochs) through a dataset
    % \item Dl is a subset of machine learning algorithms based on neural networks
    \item for accuracy, all data is read exactly once per one epoch and is done in random reads(I/O intensive)
    % \item passing it through all the layers to calculate a loss (forward pass)
    % \item use calculated loss to update the learnable parameters of the network (backpropagation)
    % \item SGD is an optimizer for loss function minimization widely used for its lower computation compared to working through the whole dataset 
    %\item increasing batch-size in the last years, from the usual range of 32-256 \cite{nvme}
    \item DL is usually I/O-bound [need source], due to the use of accelerators (GPU), size of the data and random reads
    % \item pytorch is a DL framework with some particularities (Dataloader, ...)
    \item Tensorflow is also a DL framework, with other particularities...
    % \item Imagenet
    % \item Distributed DNN training (data parallelism)
    % \item checkpointing involves saving the model state
    % \item in pytorch its done explicitly with torch.save() and in official workloads is done in-between epochs
    \item eBPF's
\end{itemize}

\section{Related work}



\begin{itemize}
    \item papers que usam darshan/tf-darshan, como per-file statistics, para caracterizar padrões \cite{9229605} \cite{zoomin} 
    \item MLPerf Storage/tese de um aluno da Oana
    \item DIO e tools de observabilidade que usam eBPF e outras (related work do DIO), LD PRELOAD, captura de de I/O request por intrumentação do código fonte.
    \item Utilizar a descrição do I/O pipeline de Tensorflow workloads para benchmark \cite{8638422}
    \item Caracterizar o I/O de LMDB, que é análoga à do PyTorch e do Tensorflow, a \textbf{database} usada como base do Caffe, baseada em \textit{mmap} e numa \textit{B+-tree}. \cite{LMDB}
    \item Comparar o overhead das leituras no treino inteiro com e sem shuffling. \cite{initial}
    \item Que métricas analisam principalmente?
    \begin{itemize}
        \item I/O skew por processo;
        \item tempo de leitura por processo;
        \item bandwidth do read por cada I/O block size;
        \item número de context switches;
        \item latência com/sem shuffle
    \end{itemize} 
    \item O que falta fazer?
    \begin{itemize}
        \item Análise empírica dos padrões de I/O como parte do processo de treino ao longo do tempo
        \item Análise da cache como interveniente no processo de I/O
        \item Testes de Rede (para modelos distribuídos)
        \item Analisar PyTorch com o nível de detalhe que analisaram outras
        \item Analisar kernel-level I/O calls 
    \end{itemize}
\end{itemize}

\section{Design}
    The system is composed of three main components:
    \begin{itemize}
        \item \textit{Collector}. Allows collecting system information during model training.
        \item \textit{BackEnd}. Offers a method for storing, searching and analyzing the raw data collected.
        \item \textit{Visualizer}. Provides a way to represent the data collected through different graphical formats in order to facilitate the understanding and analysis of complex logs by transforming them into more intuitive visual representations.
    \end{itemize}
    %\textit{Parcer}. Responsible for agreggating the collected data into usable forms, the Parcer is divided into two submodules, the parcer, that aggregates the data collected andd the plotter which  

\section{Implementation}
    The collector was implemented using (eBPF's maybe por mais especifico) and ran using GNU screen, the Parcer eas imlemented using python and ploted with the matplotlib and plotly python libraries 

\section{Evaluation}
\subsection{Experimental Setup}

We analyzed the training of the resnet50 \cite{resnet50} and alexnet \cite{alexnet} models
provided by \textit{PyTorch} on a machine with a Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz with
8 physical cores and 16 virtual cores and 16Gi of RAM and TU104 NVIDIA GPU


\subsection{Methodology}

\begin{itemize}
    \item dstat, nvidia-smi to get cost of using the tool
    \item python parser and plots
    \item grafana dashboard to get data 
\end{itemize}

\subsection{Results}

\section{Conclusion}

\bibliography{IEEEabrv, refs}

\end{document}

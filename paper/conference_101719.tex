\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\bibliographystyle{IEEEtran}

\begin{document}

\title{Characterization of Patterns in Deep Learning Models using eBPF\\
}

\author{\IEEEauthorblockN{1\textsuperscript{st} André Lucena Ribas Ferreira}
	\IEEEauthorblockA{
		\textit{University of Minho}\\
		Braga, Portugal \\
		pg52672@uminho.pt}
	\and
	\IEEEauthorblockN{1\textsuperscript{st} Carlos Eduardo da Silva Machado}
	\IEEEauthorblockA{
		\textit{University of Minho}\\
		Braga, Portugal \\
		pg52675@uminho.pt}
	\and
	\IEEEauthorblockN{1\textsuperscript{st} Gonçalo Manuel Maia de Sousa}
	\IEEEauthorblockA{
		\textit{University of Minho}\\
		Braga, Portugal \\
		pg52682@uminho.pt}
}

\maketitle

\begin{abstract}
	This document is a model and instructions for \LaTeX.
	This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes,
	or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
	component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
\begin{itemize}
	\item Machine learning has increased in popularity
	\item Deep Learning is a subset of Machine Learning
	      \begin{itemize}
		      \item image classification
		      \item natural language processing
	      \end{itemize}
	\item studies have tried to analyze I/O patterns in DL Workflows (source)
	\item \item I/O Characterization has served to produce diverse assumptions about DL I/O behavior, like high overhead for random reads, which are commonly used to justify various types of optimization explorations (\cite{LMDB}, \cite{nvme}, \cite{beegfs}), and inform other applications based on them, like benchmarks specific to DL \cite{TFbenchmark}. Application-specific storage characterization is required to find unique and worthwhile solutions for newer performance problems.
	\item very few get down to kernel level
	\item eBPF are ...
	\item we seek to provide a tool to Characterize DL workloads using eBPF's
\end{itemize}

\section{Background}

% passing the chosen data as input through the entire network layer by layer, a method called the forward pass. Afterwards, a chosen loss function compares the difference the network's output and the known ground truth, and the gradient of this function is computed in regard to the network's weights. This step is named backpropagation. The gradients are then used to update the weights of all the layers, converging to a minimization of the loss function, through an algorithm called Gradient Descent, that uses the entire dataset as the input. Each pass of the entire dataset is called an epoch, and several are computed in order to converge to the minimum. \cite{gradient} acho que isto está muito grande e fala pouco do I/O

%However, this approach is extremely costly due to the aforementioned dimension of the dataset, so state-of-the-art algorithms use Stochastic Gradient Descent (SGD) instead, an approximation that breaks an epoch into iterations, that go through the only a subset of the dataset, called a \textit{mini-batch}, as input, ensuring that each data sample is iterated through only once \cite{sgd}. Even if the original SGD only fetches a single data sample per iteration, the number of samples has been increasing over the last years, from 32, to 256 and ultimately to the 80.000s, in order to better utilize the increasing computational power afforded by the widespread utilization of GPUs as accelerators, but at an increased I/O load per iteration \cite{nvme}.

In this section, we elaborate on both the Deep Learning algorithms we seek to analyze, with focus on their I/O description, and the eBPF tools we'll use for that end.

\subsection{Deep Learning}

Deep learning (DL) is a subset of machine learning, characterized by its bigger size and heterogeneity of both algorithms and training datasets, needed for accurate predictions \cite{gradient}. State-of-the-art DL utilizes the Stochastic Gradient Descent (SGD) method, which consists in breaking down sequential training epochs in iterations, wherein subsets of the entire dataset, called \textit{mini-batches}, are read, preprocessed in memory and fed to the algorithm. To reduce training time, this computation may be done in accelerators, like GPUs, and may also be overlapped by the parallel I/O of future batches. For maximum accuracy, each data sample is iterated through only once in each epoch and each batch is composed of random data samples.

To further accelerate the training process, the dataset can be distributed among several workers in Distributed Data Parallel Learning. The model parameters are cloned among all the workers, each of them iterates through a subset of the data, and then the gradients are averaged and shared with all the workers. This way, all replicas of the network are updated with the same gradients, ensuring all workers remain consistent with each other.

As models get larger and more nodes are used in training, especially in High Performance Computing (HPC) centers, fault tolerance becomes an important issue. To address it, it is common for the model state to be saved periodically to persistent storage. Normally, this is done at epoch boundaries \cite{checkfreq}, which stalls the algorithm, waiting for I/O, whose size depends on the dimension of the network. In case of failure, the model state can be loaded from storage and training can be restarted.

Various frameworks exist to facilitate the Implementation of DL models, and each one tackles I/O differently. \textit{Pytorch} \cite{pytorch} is an up-and-coming open source AI training library, reportedly becoming commonplace in HPC centers. It provides utilities for operating on tensors with GPU support; various customizable data loading and checkpointing specifications, like parallel data treatment by different workers; a library of ready-to-use DNN models; and support for data parallel training of AI models. Its data structure is analogous to those used in other frameworks, like Tensorflow, and common databases used for DL, like LMDB, as it is based on NumPy, which uses memory mapping for its metadata access \cite{LMDB}.

\subsection{I/O in Deep Learning}

DL's I/O patterns have been attributed as the cause for the increasing storage bottleneck found recently in these applications \cite{beegfs}. On the one hand, the usual dataset's average sample is very small, but also very numerous. For example, 75\% of ImageNet's samples are smaller than 147 KB, while the dataset itself consists of over 14 million images. Allied with the random choosing of samples for batches, these patterns don't allow for the typical optimizations of traditional storage systems, directed at large and sequential patterns. On the other hand, the number of samples of each batch has been increasing over the last years, from 32, to 256 and, in some cases, to the 80.000s, in order to better utilize the higher computational power \cite{nvme}. This comes at an increased I/O load per iteration, while also reducing the overlap between I/O and computation, as GPU computation tends to scale better than I/O performance when the input sizes are bigger \cite{TFbenchmark}.

\subsection{eBPF}

\textit{Extended Berkeley Packet Filter} (eBPF) is a technology that has attracted massive adoption by both industry and academia, with a wide range of applications such as network, storage, security and sandboxing.
The main advantages are being able to run programs in kernel without modifying the kernel source code nor rebooting the system in order to take effect \cite{eBPFSurvey}.

To facilitate the creation, compilation and execution of eBPF programs, bpftrace \cite{bpftrace}, a high-level tracing language inspired in DTrace, can be used in conjunction to a wide collection of tools, some are also available in \cite{bgreggBook}.

\section{Related work}

% Traditional HPC
HPC centers have found a need for quality and agnostic I/O pattern identification for the wide array of scientific applications that run on them. Studies have developed tools for that end, like Darshan \cite{HPCIO24/7}, that seeks to classify the per-file access statistics of different processes of a given application, while minimally impacting the system it is running on. Various more specialized approaches have built upon Darshan as a base, in order to better depict a distributed environment. These include Wang et al. \cite{zoomin} and their top-down, system-wide to single-job analysis, that considers both the center's "System Weather", using HPC's specific monitoring tools Lustre Monitoring Toolkit and Slurm Logs, and the application patterns, using Darshan; or Darshan's own HPC focused enhancement \cite{HPCIODarshan}.

% Observation through eBPFs
In terms of eBPF, there are some tools that are used for system tracing and performance analysis,
such as DIO \cite{DIO} that allow the observation of I/O interactions between the application and in-kernel storage,
\cite{CAT} which is able to correlate the flow of storage and network requests application and \cite{tracee} that provides runtime security and forensics.  

% Machine Learning analysis with eBPFs
There are already some studies that uses eBPF's capability to provide kernel-level information to understand patterns such as \cite{eBPFDLNetwork} that
leverages eBPF to monitor distributed deep learning training (DDL), investigating network bandwidth utilization in DDL.
eBPF has also been utilized to characterize machine and deep learning workloads, focusing on storage and I/O efficiency in training,
revealing how storage interactions impact overall performance \cite{OanaDL, OanaML}.

\begin{itemize}
	% \item papers que usam darshan/tf-darshan, como per-file statistics, para caracterizar padrões \cite{9229605} \cite{zoomin} 
	\item MLPerf Storage/tese de um aluno da Oana
	\item DIO e tools de observabilidade que usam eBPF e outras (related work do DIO), LD\_PRELOAD, captura de de I/O request por intrumentação do código fonte.
	      % \item Utilizar a descrição do I/O pipeline de Tensorflow workloads para benchmark \cite{8638422}
	\item Caracterizar o I/O de LMDB, que é análoga à do PyTorch e do Tensorflow, a \textbf{database} usada como base do Caffe, baseada em \textit{mmap} e numa \textit{B+-tree}. \cite{LMDB}
	\item Comparar o overhead das leituras no treino inteiro com e sem shuffling. \cite{initial}
	\item Que métricas analisam principalmente?
	      \begin{itemize}
		      \item I/O skew por processo;
		      \item tempo de leitura por processo;
		      \item bandwidth do read por cada I/O block size;
		      \item número de context switches;
		      \item latência com/sem shuffle
	      \end{itemize}
	\item O que falta fazer?
	      \begin{itemize}
		      \item Análise empírica dos padrões de I/O como parte do processo de treino ao longo do tempo
		      \item Análise da cache como interveniente no processo de I/O
		      \item Testes de Rede (para modelos distribuídos)
		      \item Analisar PyTorch com o nível de detalhe que analisaram outras
		      \item Analisar kernel-level I/O calls
	      \end{itemize}
\end{itemize}

\section{Design}
The system is composed of three main components:
\begin{itemize}
	\item \textit{Collector}. Allows collecting system information during model training.
	\item \textit{BackEnd}. Offers a method for storing, searching and analyzing the raw data collected.
	\item \textit{Visualizer}. Provides a way to represent the data collected through different graphical formats in order to facilitate the understanding and analysis of complex logs by transforming them into more intuitive visual representations.
\end{itemize}
%\textit{Parcer}. Responsible for agreggating the collected data into usable forms, the Parcer is divided into two submodules, the parcer, that aggregates the data collected andd the plotter which  

\section{Implementation}
The collector was implemented using (eBPF's maybe por mais especifico) and ran using GNU screen, the Parser was implemented using python and plotted with the matplotlib and plotly python libraries

\section{Evaluation}

\subsection{Experimental Setup}

We analyzed the training of the large network ResNet50 \cite{resnet50} and the smaller AlexNet \cite{alexnet}, whose models were
provided by \textit{PyTorch}, where they're defined with 50 layers and 13 layers, respectively. Our testing machine is equipped with an Intel® Core™ i9-9900K CPU @ 3.60GHz with
8 physical cores, 16 virtual cores, 16GiB of total RAM and an NVIDIA GeForce RTX 2070 Super TU104 GPU. The dataset we used is the ImageNet \cite{imagenet} dataset, which is originally comprised of 1.2 million sample images, with an average size of 192KiB and a total size of 240GiB. Due to storage technical limitations, we scaled the dataset down to 45GiB total size by randomly selecting images, which left an average image size of ...

\subsection{Methodology}

\begin{itemize}
	\item dstat, nvidia-smi to get cost of using the tool
	\item python parser and plots
	\item grafana dashboard to get data
\end{itemize}



\subsection{Results}

here i write text so that the following text is not indented
\textbf{Observation 1.} Training images are read concurrently by multiple processes on training with several workers.
\\
\textbf{Observation 2.} fsync() is not called by the any processes created by PyTorch, this means that troch.save() does not call fsync(), confirming \cite{checkfreq}.
\\
\textbf{Observation 3.} The vast majority of system calls performed by ResNet50 (3.7 Billion with 2 epochs and batch size 64) are sched\_yield(), these calls are all made by
two threads created by the master process.
\\
\textbf{Observation 4.} Checkpoint data was always written to persistent storage when torch.save() was called, confirmed with biolatency.
\textbf{Observation 5.} When reading from a pipe, processes will read the first byte and then read a larger chunk of data, this results in there being approximately twice as many reads as there are writes to pipes, this pattern persists through chances to the num_workers parameter in the DataLoader class.
\\
\textbf{Observation 6.} The amount of reads and writes to pipes is proportional to the amount of workers allocated in the DataLoader PyTorch class with no reads or writes to pipes being performed when num_workers is set to zero

\subsubsection{Checkpoint data persistence}
We observed that no calls to fsync() were performed by the python processes, furthermore, the calls that were performed happened either some time before or some time after the checkpoint was performed, confirming the observation made in \cite{checkfreq} that the torch.save() does not guaranty data persistence.
We observed however that in all tests performed that the checkpoint data was in fact being persisted to disk.

\subsubsection{System call usage comparison between models}
We found that the overwhelming majority(3.7B on tests with 2 epochs and batch size of 64) of system calls performed by the python processes during training of the ResNet50 model were sched\_yield(), when looking at the results segregated by thread we found that ... this is not the case for training AlexNet in the same conditions

\subsubsection{Read and Write patterns on pipes}
We analyzed the write and read patterns to pipes of the AlexNet model, we found approximately twice as many reads from pipes as there were writes, upon looking into the pattern we noticed that
each time a pipe was read from, there was an initial read of 4 bytes followed by a subsequent read of more bytes.
This pattern remained with variations of the num\_workers parameter for the PyTorch DataLoader class, although the number of operations on pipes reduced in proportion to the number of workers. Conclusively, when this parameter was set to zero workers, there were no reads or writes to pipes.

\subsubsection{Pattern of block I/O access}
During the first epoch we found numerous initial reads followed by drop up until the moment the second epoch begins, after which there is a uniform pattern of read's.
This pattern was consistent with changes to the number of epochs, but the length of the initial burst of read's increased as the number of data loading workers decreased (no workers??).
This pattern also maintained when the operating systems caches were dropped in between epochs

\section{Conclusion}

\bibliography{IEEEabrv, refs}

\end{document}
